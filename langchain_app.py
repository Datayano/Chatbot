"""
Application d'assistant culinaire utilisant LangChain et RAG (Retrieval Augmented Generation)

Ce script impl√©mente un assistant culinaire intelligent qui utilise:
- LangChain: Framework pour d√©velopper des applications bas√©es sur les LLM (Large Language Models)
- RAG: Technique permettant d'enrichir les r√©ponses du LLM avec des donn√©es externes
- Streamlit: Biblioth√®que pour cr√©er des interfaces web interactives en Python

L'assistant peut:
- Recommander des recettes bas√©es sur les ingr√©dients ou pr√©f√©rences
- R√©pondre aux questions culinaires en utilisant une base de connaissances
- Formater les r√©ponses en markdown pour une meilleure lisibilit√©
"""

# Import des biblioth√®ques n√©cessaires
from langchain_community.vectorstores import Chroma  # Pour la base de donn√©es vectorielle
from langchain_openai import OpenAIEmbeddings     # Pour convertir le texte en vecteurs
from langchain_groq import ChatGroq              # LLM de Groq (alternative √† GPT)
from langchain.memory import ConversationBufferMemory  # Pour g√©rer l'historique des conversations
from langchain.chains import ConversationalRetrievalChain  # Pour combiner recherche et conversation
from langchain.prompts import PromptTemplate    # Pour structurer les prompts
from dotenv import load_dotenv  # Pour g√©rer les variables d'environnement
import streamlit as st  # Pour l'interface utilisateur
import os

# Chargement des variables d'environnement (cl√©s API, etc.)
load_dotenv()

# Initialisation du mod√®le d'embedding
@st.cache_resource  # Cache la ressource pour √©viter de la recharger √† chaque requ√™te
def get_embeddings():
    """
    Initialise le mod√®le d'embedding d'OpenAI.
    
    Les embeddings sont des repr√©sentations vectorielles du texte qui permettent
    de mesurer la similarit√© s√©mantique entre diff√©rents textes.
    
    Returns:
        OpenAIEmbeddings: Instance du mod√®le d'embedding configur√©
    """
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("OPENAI_API_KEY non trouv√©e dans les variables d'environnement")
    return OpenAIEmbeddings(
        model="text-embedding-3-small"  # Mod√®le plus l√©ger et √©conomique
    )

# Initialisation du mod√®le de langage (LLM)
@st.cache_resource
def get_llm():
    """
    Initialise le mod√®le de langage Groq.
    
    Groq est une alternative √† GPT d'OpenAI, offrant des performances similaires
    avec potentiellement des co√ªts diff√©rents.
    
    Returns:
        ChatGroq: Instance du mod√®le de langage configur√©
    """
    if not os.getenv("GROQ_API_KEY"):
        raise ValueError("GROQ_API_KEY non trouv√©e dans les variables d'environnement")
    return ChatGroq(
        temperature=0.7,  # Contr√¥le la cr√©ativit√© des r√©ponses (0=conservateur, 1=cr√©atif)
        model_name="mixtral-8x7b-32768",  # Mod√®le Mixtral, un des plus performants de Groq
        max_tokens=32768  # Longueur maximale des r√©ponses
    )

# Chargement de la base de donn√©es vectorielle
@st.cache_resource
def get_vectorstore():
    """
    Charge la base de donn√©es vectorielle Chroma.
    
    Chroma est une base de donn√©es vectorielle qui stocke les embeddings des documents
    et permet de faire des recherches par similarit√© s√©mantique.
    
    Returns:
        Chroma: Instance de la base de donn√©es vectorielle
    """
    embeddings = get_embeddings()
    vectorstore = Chroma(
        persist_directory="chroma_db",  # Dossier o√π sont stock√©s les vecteurs
        embedding_function=embeddings
    )
    return vectorstore

# Cr√©ation de la cha√Æne de conversation
def get_conversation_chain(vectorstore):
    """
    Configure la cha√Æne de conversation qui combine recherche et dialogue.
    
    Cette fonction:
    1. Initialise la m√©moire pour garder le contexte de la conversation
    2. Cr√©e un template de prompt qui guide le comportement de l'assistant
    3. Configure la cha√Æne de conversation qui utilise le LLM et la recherche
    
    Args:
        vectorstore: Base de donn√©es vectorielle pour la recherche
        
    Returns:
        ConversationalRetrievalChain: Cha√Æne de conversation configur√©e
    """
    # Initialisation de la m√©moire pour le contexte
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )
    
    # Cr√©ation du template de prompt
    template = """Tu es un assistant culinaire sympathique et comp√©tent. Utilise les √©l√©ments de contexte suivants pour 
    fournir des recommandations de recettes et des conseils de cuisine utiles. 
    
    Pour chaque recette, utilise ce format markdown:
    ### üçΩÔ∏è [Nom de la recette]
    **‚è±Ô∏è Temps de cuisson:** [temps]
    **üìä Difficult√©:** [niveau]
    
    #### ü•ò Ingr√©dients
    - [ingr√©dient 1]
    - [ingr√©dient 2]
    ...
    
    #### üìù Instructions
    [instructions d√©taill√©es]
    
    ---
    
    Pour les questions g√©n√©rales sur la cuisine, utilise du markdown avec des titres (##, ###), 
    des listes (- ou *), et du texte en gras (**) ou en italique (*) quand c'est appropri√©.
    R√©ponds toujours en fran√ßais.

    <contexte> 
    {context}
    </contexte>
    
    Historique de conversation: {chat_history}
    
    Humain: {question}
    
    Assistant: Je vais t'aider avec √ßa."""
    
    prompt = PromptTemplate(
        template=template,
        input_variables=["context", "chat_history", "question"]
    )
    
    # Configuration de la cha√Æne de conversation
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=get_llm(),
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),  # R√©cup√®re les 3 documents les plus pertinents
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt}
    )
    
    return conversation_chain

def main():
    """
    Fonction principale qui configure et lance l'interface utilisateur Streamlit.
    
    Cette fonction:
    1. Configure la page Streamlit
    2. Initialise les composants n√©cessaires (vectorstore, conversation)
    3. G√®re l'interface utilisateur et les interactions
    """
    # Configuration de la page Streamlit
    st.set_page_config(
        page_title="Assistant Culinaire",
        page_icon="üç≥",
        layout="wide"
    )
    
    st.title("üßë‚Äçüç≥ Assistant Culinaire LangChain")
    
    # Styles CSS pour l'interface
    st.markdown("""
    <style>
    .recipe-card {
        padding: 20px;
        border-radius: 10px;
        background-color: #f0f7ff;
        margin: 10px 0;
        border: 1px solid #cce0ff;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .ingredient-list {
        background-color: #ffffff;
        padding: 15px;
        border-radius: 5px;
        border: 1px solid #e0e0e0;
    }
    .recipe-header {
        color: #2c5282;
        font-size: 1.5em;
        margin-bottom: 15px;
    }
    .recipe-metadata {
        display: inline-block;
        margin: 5px 10px;
        padding: 5px 10px;
        background-color: #ffffff;
        border-radius: 15px;
        border: 1px solid #e0e0e0;
    }
    .instructions-box {
        background-color: #ffffff;
        padding: 15px;
        border-radius: 5px;
        border: 1px solid #e0e0e0;
        margin-top: 10px;
    }
    .chat-message {
        padding: 10px;
        margin: 5px 0;
        border-radius: 5px;
    }
    .assistant-message {
        background-color: #f5f5f5;
        margin-left: 5%;
        margin-right: 20%;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Initialisation des variables de session
    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    # Initialisation des composants
    try:
        # Chargement de la base de donn√©es vectorielle
        vectorstore = get_vectorstore()
        if st.session_state.conversation is None:
            st.session_state.conversation = get_conversation_chain(vectorstore)
        
        # Interface de chat
        st.markdown("""
        ### üëã Bienvenue sur votre Assistant Culinaire IA !
        Je peux vous aider √† trouver des recettes, donner des conseils de cuisine et r√©pondre √† vos questions culinaires.
        Essayez de me demander par exemple :
        - "Je veux cuisiner quelque chose avec du poulet et des l√©gumes"
        - "Quelle est une recette rapide de p√¢tes ?"
        - "Donne-moi une recette pour un d√Æner v√©g√©tarien"
        """)
        
        # Champ de saisie utilisateur
        user_input = st.text_input(
            "Que souhaitez-vous cuisiner aujourd'hui ?",
            key="user_input", 
            placeholder="Posez-moi n'importe quelle question sur la cuisine ou les recettes !"
        )
        
        if user_input:
            # Traitement de la requ√™te utilisateur
            with st.spinner('Recherche de la recette parfaite...'):
                response = st.session_state.conversation({
                    "question": user_input,
                    "chat_history": st.session_state.chat_history
                })
                
                # Mise √† jour de l'historique
                st.session_state.chat_history.append((user_input, response["answer"]))

            # Affichage de la r√©ponse avec formatage markdown
            st.markdown('<div class="chat-message assistant-message">', unsafe_allow_html=True)
            st.markdown(response["answer"])
            st.markdown('</div>', unsafe_allow_html=True)
            
            # Affichage de l'historique des conversations dans un expander
            with st.expander("üí¨ Historique des conversations", expanded=False):
                st.markdown("""
                <style>
                .chat-message-user {
                    background-color: #e3f2fd;
                    border-radius: 10px;
                    padding: 10px;
                    margin: 5px 20% 5px 0;
                }
                .chat-message-assistant {
                    background-color: #f5f5f5;
                    border-radius: 10px;
                    padding: 10px;
                    margin: 5px 0 5px 20%;
                }
                .timestamp {
                    font-size: 0.8em;
                    color: #666;
                    margin-bottom: 5px;
                }
                </style>
                """, unsafe_allow_html=True)
                
                for i, (user_msg, ai_msg) in enumerate(reversed(st.session_state.chat_history)):
                    # Message utilisateur
                    st.markdown(f'<div class="chat-message-user">', unsafe_allow_html=True)
                    st.markdown(f"**üë§ Vous:** {user_msg}", unsafe_allow_html=True)
                    st.markdown('</div>', unsafe_allow_html=True)
                    
                    # Message assistant
                    st.markdown(f'<div class="chat-message-assistant">', unsafe_allow_html=True)
                    st.markdown(f"**ü§ñ Assistant:** {ai_msg}", unsafe_allow_html=True)
                    st.markdown('</div>', unsafe_allow_html=True)
                    
                    # S√©parateur entre les conversations
                    if i < len(st.session_state.chat_history) - 1:
                        st.markdown("<hr style='margin: 15px 0; border: none; border-top: 1px solid #eee;'>", unsafe_allow_html=True)
            
    except Exception as e:
        # Gestion des erreurs
        st.error("‚ö†Ô∏è Erreur de Configuration")
        st.error(f"Une erreur s'est produite : {str(e)}")
        st.markdown("""
        Veuillez v√©rifier :
        1. Que toutes les cl√©s API requises sont d√©finies dans le fichier `.env`
        2. Que le vectorstore a √©t√© g√©n√©r√© en utilisant `generate_vectorstore.py`
        3. Que les d√©pendances n√©cessaires sont install√©es
        """)

if __name__ == "__main__":
    main()
