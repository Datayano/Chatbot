"""
Application d'assistant culinaire utilisant LangChain et RAG (Retrieval Augmented Generation)

Ce script impl√©mente un assistant culinaire intelligent qui utilise:
- LangChain: Framework pour d√©velopper des applications bas√©es sur les LLM (Large Language Models)
- RAG: Technique permettant d'enrichir les r√©ponses du LLM avec des donn√©es externes
- Streamlit: Biblioth√®que pour cr√©er des interfaces web interactives en Python

L'assistant peut:
- Recommander des recettes bas√©es sur les ingr√©dients ou pr√©f√©rences
- R√©pondre aux questions culinaires en utilisant une base de connaissances
- Formater les r√©ponses en markdown pour une meilleure lisibilit√©
"""

# Import des biblioth√®ques n√©cessaires
from langchain_community.vectorstores import Chroma  # Pour la base de donn√©es vectorielle
from langchain_openai import OpenAIEmbeddings     # Pour convertir le texte en vecteurs
from openai import OpenAI                        # Client OpenAI pour Grok
from langchain_openai import ChatOpenAI          # Pour utiliser Grok avec LangChain
from langchain.memory import ConversationBufferMemory  # Pour g√©rer l'historique des conversations
from langchain_core.prompts import PromptTemplate    # Pour structurer les prompts
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain  # Pour combiner recherche et conversation
from dotenv import load_dotenv  # Pour g√©rer les variables d'environnement
import streamlit as st  # Pour l'interface utilisateur
import os

# Chargement des variables d'environnement (cl√©s API, etc.)
load_dotenv()

# Initialisation du mod√®le d'embedding
@st.cache_resource  # Cache la ressource pour √©viter de la recharger √† chaque requ√™te
def get_embeddings():
    """
    Initialise le mod√®le d'embedding d'OpenAI.
    
    Les embeddings sont des repr√©sentations vectorielles du texte qui permettent
    de mesurer la similarit√© s√©mantique entre diff√©rents textes.
    
    Returns:
        OpenAIEmbeddings: Instance du mod√®le d'embedding configur√©
    """
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("OPENAI_API_KEY non trouv√©e dans les variables d'environnement")
    return OpenAIEmbeddings(
        model="text-embedding-3-small"  # Mod√®le plus l√©ger et √©conomique
    )

# Initialisation du mod√®le de langage (LLM)
@st.cache_resource
def get_llm():
    """
    Initialise le mod√®le de langage Grok.
    
    Grok est un mod√®le de langage d√©velopp√© par xAI,
    offrant des performances de haut niveau pour la g√©n√©ration de texte.
    
    Returns:
        ChatOpenAI: Instance du mod√®le de langage configur√©
    """
    if not os.getenv("XAI_API_KEY"):
        raise ValueError("XAI_API_KEY non trouv√©e dans les variables d'environnement")
    
    return ChatOpenAI(
        temperature=0.7,  # Contr√¥le la cr√©ativit√© des r√©ponses (0=conservateur, 1=cr√©atif)
        model_name="grok-2-1212",  # Mod√®le Grok
        api_key=os.getenv("XAI_API_KEY"),
        base_url="https://api.x.ai/v1"
    )

# Chargement de la base de donn√©es vectorielle
@st.cache_resource
def get_vectorstore():
    """
    Charge la base de donn√©es vectorielle Chroma.
    
    Chroma est une base de donn√©es vectorielle qui stocke les embeddings des documents
    et permet de faire des recherches par similarit√© s√©mantique.
    
    Returns:
        Chroma: Instance de la base de donn√©es vectorielle
    """
    embeddings = get_embeddings()
    vectorstore = Chroma(
        persist_directory="chroma_db",  # Dossier o√π sont stock√©s les vecteurs
        embedding_function=embeddings
    )
    return vectorstore

# Cr√©ation de la cha√Æne de conversation
def get_conversation_chain(vectorstore):
    """
    Configure la cha√Æne de conversation qui combine recherche et dialogue.
    
    Cette fonction:
    1. Initialise la m√©moire pour garder le contexte de la conversation
    2. Cr√©e un template de prompt qui guide le comportement de l'assistant
    3. Configure la cha√Æne de conversation qui utilise le LLM et la recherche
    
    Args:
        vectorstore: Base de donn√©es vectorielle pour la recherche
        
    Returns:
        ConversationalRetrievalChain: Cha√Æne de conversation configur√©e
    """
    # Initialisation de la m√©moire pour le contexte
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )
    
    # Cr√©ation du template de prompt
    template = """Tu es un assistant culinaire sympathique et comp√©tent. Utilise les √©l√©ments de contexte suivants pour 
    fournir des recommandations de recettes et des conseils de cuisine utiles. 
    
    Pour chaque recette, utilise ce format markdown:
    ### üçΩÔ∏è [Nom de la recette]
    **‚è±Ô∏è Temps de cuisson:** [temps]
    **üìä Difficult√©:** [niveau]
    
    #### ü•ò Ingr√©dients
    - [ingr√©dient 1]
    - [ingr√©dient 2]
    ...
    
    #### üìù Instructions
    [instructions d√©taill√©es]
    
    ---
    
    Instructions importantes:
    - Ne jamais inclure d'URLs ou de liens vers des sites externes dans tes r√©ponses
    - R√©ponds toujours en fran√ßais
    - Utilise du markdown avec des titres (##, ###), des listes (- ou *), et du texte en gras (**) ou en italique (*) quand c'est appropri√©

    <contexte> 
    {context}
    </contexte>
    
    Historique de conversation: {chat_history}
    
    Humain: {question}
    
    Assistant: Je vais t'aider avec √ßa."""
    
    prompt = PromptTemplate(
        template=template,
        input_variables=["context", "chat_history", "question"]
    )
    
    # Configuration de la cha√Æne de conversation
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=get_llm(),
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),  # R√©cup√®re les 3 documents les plus pertinents
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt}
    )
    
    return conversation_chain

def main():
    """
    Fonction principale qui configure et lance l'interface utilisateur Streamlit.
    
    Cette fonction:
    1. Configure la page Streamlit
    2. Initialise les composants n√©cessaires (vectorstore, conversation)
    3. G√®re l'interface utilisateur et les interactions
    """
    # Configuration de la page Streamlit
    st.set_page_config(
        page_title="Assistant Culinaire",
        page_icon="üç≥",
        layout="wide"
    )
    
    st.title("üßë‚Äçüç≥ Assistant Culinaire LangChain")
    
    # Chargement du CSS depuis le fichier externe
    with open('style.css') as f:
        st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)
    
    # Initialisation des variables de session
    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    # Initialisation des composants
    try:
        # Chargement de la base de donn√©es vectorielle
        vectorstore = get_vectorstore()
        if st.session_state.conversation is None:
            st.session_state.conversation = get_conversation_chain(vectorstore)
        
        # Interface de chat
        st.markdown("""
        ### üëã Bienvenue sur votre Assistant Culinaire IA !
        Je peux vous aider √† trouver des recettes, donner des conseils de cuisine et r√©pondre √† vos questions culinaires.
        Essayez de me demander par exemple :
        - "Je veux cuisiner quelque chose avec du poulet et des l√©gumes"
        - "Quelle est une recette rapide de p√¢tes ?"
        - "Donne-moi une recette pour un d√Æner v√©g√©tarien"
        """)
        
        # Champ de saisie utilisateur
        user_input = st.text_input(
            "Que souhaitez-vous cuisiner aujourd'hui ?",
            key="user_input", 
            placeholder="Posez-moi n'importe quelle question sur la cuisine ou les recettes !"
        )
        
        if user_input:
            # Traitement de la requ√™te utilisateur
            with st.spinner('Recherche de la recette parfaite...'):
                response = st.session_state.conversation.invoke({
                    "question": user_input,
                    "chat_history": st.session_state.chat_history
                })
                
                # Mise √† jour de l'historique
                st.session_state.chat_history.append((user_input, response["answer"]))

            # Affichage de la r√©ponse avec formatage markdown
            st.markdown('<div class="chat-message assistant-message">', unsafe_allow_html=True)
            st.markdown(response["answer"])
            st.markdown('</div>', unsafe_allow_html=True)
            
            # Affichage de l'historique des conversations dans un expander
            with st.expander("üí¨ Historique des conversations", expanded=False):
                for i, (user_msg, ai_msg) in enumerate(reversed(st.session_state.chat_history)):
                    # Message utilisateur
                    st.markdown(f'<div class="chat-message-user">', unsafe_allow_html=True)
                    st.markdown(f"**üë§ Vous:** {user_msg}", unsafe_allow_html=True)
                    st.markdown('</div>', unsafe_allow_html=True)
                    
                    # Message assistant
                    st.markdown(f'<div class="chat-message-assistant">', unsafe_allow_html=True)
                    st.markdown(f"**ü§ñ Assistant:** {ai_msg}", unsafe_allow_html=True)
                    st.markdown('</div>', unsafe_allow_html=True)
                    
                    # S√©parateur entre les conversations
                    if i < len(st.session_state.chat_history) - 1:
                        st.markdown("<hr style='margin: 15px 0; border: none; border-top: 1px solid #eee;'>", unsafe_allow_html=True)
            
    except Exception as e:
        # Gestion des erreurs
        st.error("‚ö†Ô∏è Erreur de Configuration")
        st.error(f"Une erreur s'est produite : {str(e)}")
        st.markdown("""
        Veuillez v√©rifier :
        1. Que toutes les cl√©s API requises sont d√©finies dans le fichier `.env`
        2. Que le vectorstore a √©t√© g√©n√©r√© en utilisant `generate_vectorstore.py`
        3. Que les d√©pendances n√©cessaires sont install√©es
        """)

if __name__ == "__main__":
    main()
